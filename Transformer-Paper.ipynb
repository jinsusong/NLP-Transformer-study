{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Transformer.ipynb",
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyOUvPmf/nZEluRSU6UBZBd/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jinsusong/NLP-Transformer-study/blob/main/Transformer-Paper.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Transformer : Attention is All You Need"
      ],
      "metadata": {
        "id": "84pSvpZ_7DZ7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2021년 기준으로 최신 고성능 모델들은 Transformer 아키텍처를 기반으로 함 \n",
        "\n",
        "GPT : Transformer의 디코더 아키텍처를 활용\n",
        "BERT : Transformer의 인코더 아키텍처를 활용\n",
        "\n"
      ],
      "metadata": {
        "id": "_6sT2chJ7Ict"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. RNN\n",
        "2. LSTM\n",
        "3. Seq2Seq : 고정된 크기의 context vector 활용 \n",
        "4. Attention : Seq2Seq모델에 Attention 기법을 추가해 성능을 끌어올림\n",
        "5. Transformer : RNN 사용 안하고, Attention만 사용하니까 성능이 훨씬 좋아짐\n",
        "6. GPT-1 \n",
        "7. BERT\n",
        "8. GPT-3 \n"
      ],
      "metadata": {
        "id": "iQhB9fwY7adp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "기존 Seq2Seq 모델의 한계점 \n",
        "- context vector 에 소스 문장의 정보를 압축하기 때문에 병목현상이 발생하여 성능 하락의 원인이 된다. \n",
        "\n"
      ],
      "metadata": {
        "id": "cavhaKC-7jMC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "문제 상황 : 하나의 컨텍스트 벡터가 소스 문장의 모든 정보를 가지고 있어야 하므로 성능이 저하됨 \n",
        "\n",
        "해결 방안 : 매번 소스 문장에서의 출력 전부를 입력으로 받으면 어떻게 될까? \n",
        "최신 GPU는 많은 메모리와 빠른 병렬 처리를 지원함 \n"
      ],
      "metadata": {
        "id": "YxlV6bfx8MJv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Seq2Seq with Attention\n",
        "\n",
        "Seq2Seq 모델에 어텐션(Attention)매커니즘을 사용\n",
        "디코더는 인코더의 모든 출력을 참고"
      ],
      "metadata": {
        "id": "uM3yDS2E-y40"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Seq2Seq with Attention: 디코더(Decoder)"
      ],
      "metadata": {
        "id": "1EoRYNBs_CMS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "디코더는 매번 인코더의 모든 출력 중에서 어떤 정보가 중요한지를 계산함\n",
        " - 에너지 : 소스문장에서 나온 출력값 중 연관도를 수치로 표현 \n",
        " - 가중치 : 수치로 표현된 에너지 값을 소프트맥스에 넣어 상대적인 확률로 표현\n",
        "\n",
        " - Decoder의 입력은 (가중치에 소스문장 스테이트를 곱한값을) 모두 더한 값 \n",
        "Decoder = Sum(가중치 * 소스문장 state)\n",
        "\n"
      ],
      "metadata": {
        "id": "BqNGGlXu_69n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 트랜스포머(Transformer)\n",
        " - 어텐션 기법만 잘 활용해도 자연어 처리 네트워크에서 좋은 성능을 얻을 수 있다. \n",
        " - 트랜스포커는 RNN이나 CNN을 전혀 필요로 하지 않는다. "
      ],
      "metadata": {
        "id": "rsUWOMn6CMN9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " 1. 트랜스포머는 RNN이나 CNN을 사용하지 않는다. 대신 Position Encoding을 사용함(순서에 대한 정보를 줌)\n",
        " 2. BERT나 GTP에서도 사용함 \n",
        " 3. 인코더와 디코더로 구성됨 \n",
        " 4. Attention 과정을 여러 레이어에서 반복함.\n",
        " "
      ],
      "metadata": {
        "id": "U4xrrJVWCcMh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "트랜스포머의 동작 원리\n",
        "\n",
        "1. 입력 값 임베딩(Embedding)\n",
        " - 기존 임베딩은 네트워크에 값을 넣기 전에 입력 값을 임베딩으로 표현하기 위해 사용하는 레이어 \n",
        " - RNN 기반 아키텍처를 사용하면 각각의 단어가 입력될 때 순서대로 입력이 되기 때문에 자동으로 각각의 hidden_state값은 순서에 대한 정보를 가짐 \n",
        " - 트랜스포머와 같이 RNN 기반이 아닌 아키텍처는 문장에서 단어의 위치에 대한 정보를 주기 위해서 임베딩을 사용해야 함.\n",
        " - 이를 위해 트랜스포머에서는 Positional Encoding을 사용\n",
        " - Input Embedding Matrix와 같은 크기를 가지는 Positional Encoding 정보를 넣어줘서 각각의 단어가 어떤 순서를 가지는지 네트워크에게 알려줌 \n",
        " \n",
        "2. 인코더 (Encoder)\n",
        " - 임베딩이 끝난 이후에 어텐션을 진행(Multi-head Attention)\n",
        " - Input Embedding Matrix + Positional Encoding  -> Multi-head Attention\n",
        " - 입력을 받아서 각각의 단어들을 이용해서 Attention을 수행 \n",
        " - 인코더에서 사용하는 어텐션은 Self-Attention으로 \n",
        " - 입력값 각각의 단어가 서로에게 어떤 연관성을 가지고 있는지를 구하기 위해 사용\n",
        " - 문맥에 대한 정보를 학습하기 위해 사용\n",
        " - 추가적으로 잔여 학습(Residual Learning)을 사용\n",
        " - Residual Learning : 특정 레이어를 건너뛰고 입력할 수 있도록 만드는 것 \n",
        " - 전체 네트워크는 기존 정보를 입력 받으면서 추가적으로 잔여된 부분만 학습하도록 만들기 때문에 전반적인 학습적인 난이도가 낮고 초기 모델 수렴 속도가 높게 되고 그로 인해 글로벌 옵티멀을 찾을 확률이 높아져 다양한 네트워크에 대해서 잔여 학습을 사용했을 때 성능이 좋아지는걸 확인할 수 있음.\n",
        " - 어텐션과 정규화 과정을 반복 \n",
        " - Multi-head Attention -> Add + Norm -> Feedforward Layer -> Add + Norm \n",
        " - 각 레이어는 서로 다른 파라미터를 가짐\n",
        "\n",
        "3. 인코더와 디코더 \n",
        " - 여러개의 인코더 레이어를 반복해서 마지막에 나오는 인코더 출력값은 디코더에 들어가게됨 \n",
        " - Seq2Seq 모델에 Attention 매커니즘을 활용했을때와 마찬가지로 디코더에서는 매번 출력할 때 마다 입력 소스 문장 중에서 어떤 단어에게 가장 많은 초점을 둬야하는지 알려주기위함 \n",
        " - 디코더도 마찬가지로 여러개 레이어로 구성되고 마지막 레이어에서 나오는 출력값이 실제로 번역을 수행하는 결과(출력 단어) \n",
        " - 이때 각각의 디코더 레이어는 인코더의 마지막 레이어의 출력값을 입력으로 받음 \n",
        "\n",
        "4. 디코더\n",
        " - 각각 단어 정보를 받아서 상대적인 Positional Encoding값을 추가한 뒤에 입력을 넣는다. \n",
        " - 하나의 디코더 레이어에는 두 개의 어텐션을 사용\n",
        " - 첫번째 어텐션은 셀프 어텐션으로 인코더와 마찬가지로 각각의 단어들이 서로가 서로에게 어떤 가중치를 가지는지 구함\n",
        " - 이어서 두번째 어텐션에서는 인코더에 대한 정보를 어텐션할 수 있도록 만듬 \n",
        " - 각각의 출력정보가 인코더의 출력정보를 받아와 사용할 수 있도록 함 \n",
        " - 다시 말해 각각의 출력되고 있는 단어가 소스 문장에서의 어떤 단어와 연관이 있는지 구해줌\n",
        " - 두번째 어텐션은 인코더-디코더 어텐션이라고 부름 \n",
        " - 디코더 레이어를 여러번 사용\n",
        " \n",
        "- 트랜스포머에서는 마지막 인코더의 출력이 모든 디코더 레이어에 입력이 된다.\n",
        "\n"
      ],
      "metadata": {
        "id": "Ddnr2NAQCKTO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "트랜스포머의 어텐션 동작 원리 : Multi-Head Attention Layer\n",
        "\n",
        "1. 인코더와 디코더는 Multi-Head Attention레이어를 사용\n",
        " - 어텐션을 위한 세 가지 입력 요소 \n",
        " - 쿼리(Query), 키(Key), 값(Value) \n",
        " - Scaled Dot-Product Attention\n",
        " - Concat : 입력값과 출력값의 dim을 같게 하기 위해 일렬로 쭉 붙임\n",
        " - Linear : \n",
        " - output값 \n",
        "\n",
        "2. Scaled Dot-Product Attention : 어테션 매커니즘은 어떠한 단어가 다른 단어들과 어떠한 연관성을 가지는지를 구하는 것 \n",
        " - 물어보는 주체 : 쿼리\n",
        " - 물어보는 대상 : 키 \n",
        " - 쿼리(물어보는 주체)랑 키(어텐션을 수행할 단어) 가 들어가면 (MatMul)행렬 곱을 수행한 뒤에 Scale -> Mask -> SoftMax 를 취해서 각각의 키 중에서 어떤 단어와 가장 높은 연관성을 가지는지 비율을 구함 \n",
        " - 구해진 확률값과 Value값을 곱해서 가중치가 적용된 결과적인 Attention Value가 구해짐 \n",
        " - \n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "7KP4sgM1K-xt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "트랜스포머의 동작 원리 : Query, Key, Value \n",
        " - 어텐션을 위해 Query, Key, Value 값이 필요 \n",
        " - 각 단어의 임베딩을 이용해 생성할 수 있음 .\n",
        "\n",
        "\n",
        "트랜스포머의 동작 원리 : Scaled Dot-Product Attention\n",
        " - 마스크 행렬을 이용해 특정 단어는 무시할 수 있도록 함.\n",
        " - 마스크 값으로 음수 무한의 값을 넣어 softmax함수의 출력이 0%에 가까워지도록 함.\n",
        "\n",
        "트랜스포머의 동작 원리 : 어텐션의 종류 \n",
        " - 트랜스포머에서는 세 가지 종류의 어텐션 레이어가 사용됨\n",
        " - Encoder Self- : 서로에게 어떤 연관성이 있는지\n",
        " - Masked Decoder Self-Attention : 출력단어가 앞쪽에 등장한 단어만 참고하도록 함 \n",
        " - Encoder-Decoder Attention : Query 디코더에 있고 Key와 Value는 인코더에 있는 상황, 출력 단어가 입력 단어에서 어떤 단어에 가중치가 가장 큰지 구할 수 있어야 하기 때문에 디코더에 있는 쿼리값이 인코더의 키, 벨류를 참조함.\n",
        "\n",
        "\n",
        "트랜스포머의 동작 원리 : Self-Attention\n",
        " - Self-Attention은 인코더와 디코더 모두에서 사용\n",
        " - 매번 입력 문장에서 각 단어가 다른 어떤 단어와 연관성이 높은 지 계산할 수 있음.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        " "
      ],
      "metadata": {
        "id": "4BDi07_BO2Fo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "트랜스포머의 동작 원리 : Positional Encoding \n",
        " - 주기 함수를 활용한 공식을 활용\n",
        " - 각 단어의 상대적인 위치 정보를 네트워크에게 입력 \n",
        " "
      ],
      "metadata": {
        "id": "L8fgayANO-dU"
      }
    }
  ]
}