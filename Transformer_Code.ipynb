{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Transformer Code.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "background_execution": "on",
      "authorship_tag": "ABX9TyP1TfgNaWT9IuNVxBNzlYPy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jinsusong/NLP-Transformer-study/blob/main/Transformer_Code.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Transformer : Attention is All You Need"
      ],
      "metadata": {
        "id": "84pSvpZ_7DZ7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2021년 기준으로 최신 고성능 모델들은 Transformer 아키텍처를 기반으로 함 \n",
        "\n",
        "GPT : Transformer의 디코더 아키텍처를 활용\n",
        "BERT : Transformer의 인코더 아키텍처를 활용\n",
        "\n"
      ],
      "metadata": {
        "id": "_6sT2chJ7Ict"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. init() 클래스 생성할 때 실행\n",
        "    - Transformer()\n",
        "2. build() 객체가 처음으로 사용될 때 실행 \n",
        "3. call() 객체 호출 시 실행\n",
        "\n",
        "\n",
        "- Training\n",
        "    1. parameters 정의\n",
        "        - Transformer() # init() \n",
        "            - Encoder() # init() \n",
        "                - Embedding() #차원 정의 \n",
        "                - PositionalEncoding() # init() 위치 벡터 \n",
        "                - Dropout() \n",
        "                - EncoderLayer() # init() 인코더 층 설계 # for 문 사용 6개 \n",
        "                    - hidden layer # 2048 정의\n",
        "                    - multi-head # 8개 정의\n",
        "            - Decoder() # init()\n",
        "                - Encoder() 와 동일 # init() \n",
        "            - layers.Dense() 신경망 생성 \n",
        "    2. Loss 정의 \n",
        "        - tf.keras.losses.SparseCategoricalCrossentropy() # 활성화 함수 설정? \n",
        "        - tf.keras.metrics.Mean() # ??\n",
        "        - tf.keras.metrics.SparseCategoricalAccuracy() # ??? \n",
        "    3. Optimizer\n",
        "        - CustomSchedule() # init()\n",
        "        - tf.keras.optimizers.Adam() # ??? \n",
        "    4. Epochs\n",
        "        - transformer() # call()\n",
        "            - create_padding_mask() # encoder mask ??? \n",
        "            - create_padding_mask() # decoder mask_1 ??? \n",
        "            - create_look_ahead_mask() decoder mask_1 ???? \n",
        "            - create_padding_mask() # decoder mask_2 ??? \n",
        "            - encoder() # call()\n",
        "                - embedding() #\n",
        "                - tf.math.sqrt() # 3.4 multiplied sqrt of d_model ????\n",
        "                - pos_encoding() # call() 입력으로부터 dim을 얻고 pos 인코딩을 완전히 별도로 계산하여 끝에 쌓는다.\n",
        "                    - get_angles() # sin, cos 함수를 사용 위치 값을 얻음, 반환 : 위치 값 + 입력 값 \n",
        "                - encoderLayer() # build() # call()  \n",
        "                    - MultiHeadAttention() # init() \n",
        "                    - layers.LayerNormalization() # 정규화? \n",
        "                    - layers.Dense() #??? \n",
        "                    - Multi_head_attention() # build() # call()\n",
        "\n",
        "                        - query_lin # ???\n",
        "                        - key_lin  # ???\n",
        "                        - value_lin # ???\n",
        "                        - final_lin # ???\n",
        "                        - scaled_dot_product_attention() #attention computation\n",
        "                        - tf.transpose() # ??? \n",
        "                        - concat_attention () # ??? \n",
        "            - decoder() # call() \n",
        "                - #???? \n",
        "                - #???? \n",
        "                - #????\n",
        "                - #????\n",
        "                - #????\n",
        "                - #????\n",
        "        - loss_function() # ???? \n",
        "        - gradient() # ??? \n",
        "        - optimizer.apply_gradients() # ??? \n",
        "        - \n",
        "\n",
        "            \n",
        "\n",
        "                    \n",
        "\n",
        "    "
      ],
      "metadata": {
        "id": "qGriyODIpKLU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "마운트 구글 드라이브"
      ],
      "metadata": {
        "id": "EV3iB6gOB_DN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "%pwd\n",
        "\n",
        "\"\"\" \n",
        "Use this javascript code in inspect>console so you wont need to click the page every 15 min:\n",
        "\n",
        "########################\n",
        "function ConnectButton(){\n",
        "    console.log(\"Connect pushed\"); \n",
        "    document.querySelector(\"#top-toolbar > colab-connect-button\").shadowRoot.querySelector(\"#connect\").click() \n",
        "}\n",
        "setInterval(ConnectButton,60000);\n",
        "########################\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "0_3MDywzB0tW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "change current path to where the working project folder is at "
      ],
      "metadata": {
        "id": "9ZJ_yddaCgwX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%mkdir -p /content/drive/MyDrive/projects/transformers_translation/data\n",
        "%cd /content/drive/MyDrive/projects/transformers_translation/data\n",
        "#%pwd\n",
        "#%ls -al"
      ],
      "metadata": {
        "id": "7q2JK-gHCKG9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 0 : Get The Data "
      ],
      "metadata": {
        "id": "BNXzSWPXCrNf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "upload the data to our current path and unzip it (numcomment and run this only once)"
      ],
      "metadata": {
        "id": "a9SU629ICwd3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # data is from: https://www.statmt.org/europarl/ you can use this or just upload your own data\n",
        " #%cd data\n",
        "!wget https://www.statmt.org/europarl/v7/de-en.tgz\n",
        "!tar -xvf de-en.tgz\n",
        "%cd ..\n",
        "%pwd"
      ],
      "metadata": {
        "id": "Ab7v2fU2DAuO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "get non breaking prefixs"
      ],
      "metadata": {
        "id": "6-xucs7EDDW9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# get non_breaking_prefixes from https://github.com/moses-smt/mosesdecoder/tree/master/scripts/share/nonbreaking_prefixes\n",
        "# then rename them to: \"nonbreaking_prefix.en\" and \"nonbreaking_prefix.de\" and put them in your data folder so we dont consider the\n",
        "# dot in 'mr.jackson' as the end of a sentence\n",
        "%cd /content/drive/MyDrive/projects/transformers_translation/data\n",
        "%pwd \n",
        "!wget https://raw.githubusercontent.com/moses-smt/mosesdecoder/master/scripts/share/nonbreaking_prefixes/nonbreaking_prefix.de\n",
        "!wget https://raw.githubusercontent.com/moses-smt/mosesdecoder/master/scripts/share/nonbreaking_prefixes/nonbreaking_prefix.en\n",
        "%ls -al\n",
        "%pwd "
      ],
      "metadata": {
        "id": "L_vmRCdeDIUc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 1 : Importing Dependencies"
      ],
      "metadata": {
        "id": "PKIcHrvtDWnH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import math \n",
        "import re\n",
        "import time # to see how long it takes in training\n"
      ],
      "metadata": {
        "id": "jYY54u1GDci7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%tensorflow_version 2.x\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers \n",
        "import tensorflow_datasets as tfds # tools for the tokenizer \n",
        "\n"
      ],
      "metadata": {
        "id": "KNAOyxcxDjA3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 2 : Data Preprocessing "
      ],
      "metadata": {
        "id": "vxbhQ8ZfD1bF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "read files"
      ],
      "metadata": {
        "id": "lxZlpQSuU6xj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"/content/drive/MyDrive/projects/transformers_translation/data/europarl-v7.de-en.en\", mode='r', encoding=\"utf-8\") as f:\n",
        "    text_en = f.read()\n",
        "\n",
        "with open(\"/content/drive/MyDrive/projects/transformers_translation/data/europarl-v7.de-en.de\", mode='r', encoding=\"utf-8\") as f:\n",
        "    text_de = f.read()\n",
        "\n",
        "print(text_en[:50])\n",
        "print(text_de[:50])\n",
        "\n"
      ],
      "metadata": {
        "id": "FOaEgZaMU8oy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"/content/drive/MyDrive/projects/transformers_translation/data/nonbreaking_prefix.en\", mode='r', encoding=\"utf-8\") as f: \n",
        "    non_breaking_prefix_en = f.read()\n",
        "\n",
        "with open(\"/content/drive/MyDrive/projects/transformers_translation/data/nonbreaking_prefix.de\", mode='r', encoding=\"utf-8\") as f:\n",
        "    non_breaking_prefix_de = f.read()\n",
        "\n",
        "print(non_breaking_prefix_en[:5])\n",
        "print(non_breaking_prefix_de[:5])\n"
      ],
      "metadata": {
        "id": "af_Uyge_WB3q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cleaning"
      ],
      "metadata": {
        "id": "VPkHOBp7WtXf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 해석 필요 \n",
        "for prefix in non_breaking_prefix_en:\n",
        "    text_en = text_en.replace(prefix, prefix + '###')\n",
        "\n",
        "text_en = re.sub(r\"\\.(?=[0-9]|[a-z]|[A-Z])\", \".###\", text_en)\n",
        "text_en = re.sub(r\"\\.###\",'',text_en)\n",
        "text_en = re.sub(r\" +\", ' ', text_en)\n",
        "text_en = text_en.replace('###',' ')\n",
        "\n",
        "text_en = text_en.split(\"\\n\")\n",
        "\n",
        "for prefix in non_breaking_prefix_de:\n",
        "    text_de = text_de.replace(prefix, prefix + '###')\n",
        "text_de = re.sub(r\"\\.(?=[0-9]|[a-z]|[A-Z])\", \".###\", text_de)\n",
        "text_de = re.sub(r\"\\.###\",'',text_de)\n",
        "text_de = re.sub(r\" +\",' ',text_de)\n",
        "text_de = text_de.replace('###',' ')\n",
        "\n",
        "text_de = text_de.split(\"\\n\")\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "zoqLcKylWvEX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tokenizing\n"
      ],
      "metadata": {
        "id": "5AI7L1MFgjYW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer_en = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(\n",
        "    text_en, target_vocab_size=8000\n",
        ")\n",
        "\n",
        "tokenizer_de = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(\n",
        "    text_de, target_vocag_size=8000\n",
        ")\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "y4i-lX14glIx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "VOCAB_SIZE_EN = tokenizer_en.vocab_size + 2\n",
        "VOCAB_SIZE_DE = tokenizer_de.vocab_size + 2 \n",
        "\n",
        "# we put start and tokens as size-1 and size-2 which are the same as \n",
        "# tokenizer_size and tokenizer_size +1 because the words are from [0 to ts -1]\n",
        "# tokenizer_en.encode(sentence) give a list then list + list + list appends them\n",
        "\n",
        "input = [[VOCAB_SIZE_EN-2] + tokenizer_en.encode(sentence) + [VOCAB_SIZE_EN-1]\n",
        "         for sentence in text_en]\n",
        "\n",
        "outputs = [[VOCAB_SIZE_DE-2] + tokenizer_de.encode(sentence) + [VOCAB_SIZE_DE-1]\n",
        "          for sentence in text_de]\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "vo_mI4FjiEFd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Remove too long sentences\n",
        "\n",
        "- Why? (1) because when we pad we will have a hugeeee ram issuie for example sentence sizes of 1,100,2 when we pad they become 100,100,100 which we would rather loose that 100 than pad all to 100 (2) takes too much time to train"
      ],
      "metadata": {
        "id": "q0zRK-GSlFf4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_LENGTH = 20 # we will still have a lot of data with max len of 20 \n",
        "\n",
        "# this part. why we do it is a bit tricky. pay attention why we do it like this:\n",
        "idx_to_remove = [count for count, sent in enumerate(inputs)\n",
        "if len(sent) > MAX_LENGTH]\n",
        "\n",
        "# we remove in reversed because of shifting issuies when we satrt from begining\n",
        "for idx in reversed(idx_to_remive):\n",
        "    del inputs[idx]\n",
        "    del outputs[idx]\n",
        "\n",
        "# same stuff for outputs > 20 \n",
        "idx_to_remove = [count for count, sent in enumerate(outputs)\n",
        "if len(sent) > MAX_LENGTH]\n",
        "\n",
        "for idx in reversed(idx_to_remove):\n",
        "    del inputs[idx]\n",
        "    del outputs[idx]\n",
        "\n",
        " "
      ],
      "metadata": {
        "id": "kUgqSpjElLKO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### input / output creation"
      ],
      "metadata": {
        "id": "XkXUBHnZnsWt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. padding\n",
        "2. batching"
      ],
      "metadata": {
        "id": "HEcaUJNAnvL0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = tf.keras.preprocessing.sequence.pad_sequences(inputs,\n",
        "                                                       value=0,\n",
        "                                                       padding='post',\n",
        "                                                       maxlen = MAX_LENGTH)\n",
        "\n",
        "outpus = tf.keras.preprocessing.sequence.pad_sequences(outputs,\n",
        "                                                       value=0,\n",
        "                                                       padding='post',\n",
        "                                                       maxlen = MAX_LENGTH)\n",
        "\n"
      ],
      "metadata": {
        "id": "qpQ7c_ULnzMh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE =64\n",
        "BUFFER_SIZE = 20000 # how much data to keep\n",
        "\n",
        "# now we turned our data into a dataset \n",
        "dataset = tf.data.Dataset.from_tensort_slices((inputs, outputs))\n",
        "\n",
        "#this is something that improves the way the dataset is stored. it increases\n",
        "# the speed of accessing the data which increases training speed in return :\n",
        "data = dataset.cache()\n",
        "\n",
        "#now we shuffle in batches\n",
        "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
        "\n",
        "#this increases the speed even further:\n",
        "dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)\n"
      ],
      "metadata": {
        "id": "6q18koyeoso7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 3 : Model Building\n",
        "\n",
        "- A - Positional Encoding ( look at the formula in the paper)"
      ],
      "metadata": {
        "id": "CJqzmua2tLcA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEncoding(layers.Layer):\n",
        "    \n",
        "    def __init__(self):\n",
        "        # 이 위치 인코더는 레이어의 하위 항목으로 만들어 레이어가 가지고 있는 모든 속성을 가집니다\n",
        "        #  부모 클래스의 초기화 메서드 호출하기\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "\n",
        "    def get_angles(self, pos, i, d_model):\n",
        "        \"\"\"\n",
        "        :pos: (seq_len, 1) 문장 내 단어의 색인 [0 ~ 19]\n",
        "        :i: 임베딩의 치수 (각각 200 치수) 그 다음> [0 ~ 199]\n",
        "        :d_model: 내장형 크기(예: glove  크기 200)\n",
        "        :return: (seq_len, d_model) 왜? 우리는 그 단어의 모든 위치 대 모든 차원의 인코딩을 얻고 있다.\n",
        "        \"\"\"\n",
        "           # PE (pos,2i) = sin (pos / 10000^(2i / d_model) )\n",
        "        # PE (pos,2i+1) = cos (pos / 10000^(2i / d_model) )\n",
        "        angles = 1 / np.power(10000., (2*(i//2))/np.float32(d_model))\n",
        "        return pos * angles # dim: (seq_len, d_model)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        # input.shape = [batch_size, multihead_size(sz=8), each word (pos), that words embedding]\n",
        "        # 우리는 그들의 위치를 고려하여 입력값을 변경하지 않고, 우리는 단지 입력으로부터 딤을 얻고 완전히 개별적으로 pos 인코딩을 계산하여 끝에 쌓을 뿐이다.\n",
        "        seq_length = inputs.shape.as_list()[-2] # basically the pos\n",
        "        d_model = inputs.shape.as_list()[-1] # basically the embedded values\n",
        "        angles = self.get_angles(np.arange(seq_length)[:, np.newaxis],\n",
        "                                 np.arange(d_model)[np.newaxis, :],\n",
        "                                 d_model)\n",
        "        angles[:, 0::2] = np.sin(angles[:, 0::2])\n",
        "        angles[:, 1::2] = np.cos(angles[:, 1::2])\n",
        "        # 우리는 이것을 할 수 있다. 왜냐하면 입력과 인코딩이 같은 치수를 가져야 하기 때문이다. 그래서 우리는 0을 넣지 않는 새로운 축을 만들기 위해서… 모든 배치에 대해 같은 조광을 복사하고...\n",
        "        pos_encoding = angles[np.newaxis, ...]\n",
        "        # 이제 우리는 입력과 그들의 pos_encodings를 둘 다 반환해야 하지만 우리는 np에 pos_codings가 있어서 그것들을 tf로 만든다\n",
        "        return inputs + tf.cast(pos_encoding, tf.float32)\n",
        "\n"
      ],
      "metadata": {
        "id": "ixeXGTqMtZMf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### B - Attention \n",
        " - Attention computation ( see the formula in the paper)"
      ],
      "metadata": {
        "id": "8jyRV040xmYk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def scaled_dot_product_attention(queries, keys, values, mask):\n",
        "    # Q*K는 [output_len, d_model] * [d_model, input_len]이며 영어와 프랑스어 모두 20입니다.\n",
        "    # transpose_b = True는 키를 키 방향으로 돌리게 됩니다. T는 각각 이 dim : [batch_size, nb_proj, seq_len, d_proj]이므로 전치 시 [a,b,c,d] * []가 된다.\n",
        "    product = tf.matmul(queries, keys, transpose_b = True)\n",
        "    keys_dim = tf.cast(tf.shape(keys)[-1], tf.float32) # makes the dim_num float\n",
        "    scaled_product = product / tf.math.sqrt(keys_dim) # scalse it (formula stuff)\n",
        "\n",
        "    #왜냐하면 논문이 말한 이 마스크는 프로그램이 feauture를 보는 것을 막기 위해 선택적이기 때문이다. \n",
        "    #왜냐하면 우리가 역프로포즈를 할 때 그들은 그들 앞에 있는 것들을 고려할 것이기 때문에 \n",
        "    #우리는 이것을 멈추기 위해 그들에게 -1e9를 더해서 소프트맥스 후에 그들에게 0이 된다.\n",
        "\n",
        "    if mask is not None:\n",
        "        scaled_product +=(mask * -1e9)\n",
        "\n",
        "    #소프트맥스를 마지막 축을 따라 적용한다. 왜냐하면 우리는 소프트맥스의 합이 input_len에 1 scale_product = [output_len] -> softmax on input_len이 되기를 원하기 때문에 \n",
        "    #기본적으로 out_len에 대한 프로브를 동일하게 유지하지만 out_len에 대한 프로브를 찾는다.\n",
        "    probs = tf.nn.softmax(scaled_product, axis = -1)\n",
        "\n",
        "    # attention = [output_len. input_len] * [input_len, d_model] = [output_len, d_model]\n",
        "    # 그래서 이제 우리는 각각의 출력 단어에 대한 d_model 가중치를 가지고 있으며, 각각의 out_lens에 대한 예측을 보기 위해 앞으로 전달할 것이다.\n",
        "    attention = tf.matmul(probs, value)\n",
        "\n",
        "    return attention\n",
        "    "
      ],
      "metadata": {
        "id": "_ClRwS_i6iCq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import numpy as np\n",
        "# 이것은 단지 당신이 dims에 대한 코드의 줄에서 어떤 일이 일어나는지 보기 위한 테스트이다 \n",
        "#(4dimes에서 우리가 깨달은 것은 matmul이 다른 dims를 배치 크기와 다른 것으로 간주하기 때문에\n",
        "# 마지막 두 개에 대해서만 멀티를 수행하였다). (tf.matmul(a, b, transpose_b= True)\n",
        "\n",
        "# a = np.arange(24).reshape(1,2,3,4)\n",
        "# a = tf.convert_to_tensor(a, np.float32)\n",
        "# b = np.arange(24).reshape(1,2,3,4)\n",
        "# b = tf.convert_to_tensor(b, np.float32)\n",
        "# product = tf.matmul(a, b, transpose_b=True)\n",
        "# print(product.shape)"
      ],
      "metadata": {
        "id": "UBtZ9LzlEa6A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Multi-Head attention sublayer"
      ],
      "metadata": {
        "id": "0yQqg3VGEovk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(layers.Layer):\n",
        "    def __init__(self, nb_proj):\n",
        "        \"\"\"\n",
        "        :nb_proj : the number of projections for the multihead\n",
        "        \"\"\"\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.np_proj = nb_proj\n",
        "\n",
        "    \n",
        "    #이것은 init와 동일하지만 우리가 객체를 처음으로 사용할 때 발생한다, init에서는 객체를 만들 때 호출되었다.\n",
        "    def build(self, input_shape):\n",
        "        self.d_model = input_shape[-1]\n",
        "        # 우리는 그것들이 분리될 수 있는지 확인하고 싶다.\n",
        "        assert self.d_model % self.nb_proj == 0\n",
        "        # 우리는 그것을 정수로 만들기 위해 2개의 슬래시를 사용한다.\n",
        "        self.d_proj = self.d_model // self.nb_proj\n",
        "\n",
        "        self.query_lin = layers.Dense(self.d_model)\n",
        "        self.key_lin = layers.Dense(self.d_model)\n",
        "        self.value_lin = layers.Dense(self.d_model)\n",
        "        self.final_lin = layers.Dense(self.d_model)\n",
        "\n",
        "    def split_proj(self, inputs, batch_size):\n",
        "        \"\"\"\n",
        "        : inputs : [batch_size, seq_len(20), d_model(prev layer dim)]\n",
        "\n",
        "        : return: \n",
        "            dims = [batch_size, nb_proj, seq_len, d_proj]\n",
        "            nb_proj는 cnn의 채널과 같습니다. 기본적으로 d_model을 nb_proj * d_proj로 분할하여 d_model/nb_proj를 찾습니다.\n",
        "        \"\"\"        \n",
        "        new_shape = (batch_size, -1, self.nb_proj, self.d_proj)\n",
        "        #here we will get: [ batch_sz, seq_len, nb_proj, d_proj]\n",
        "\n",
        "        splited_inputs = tf.reshape(inputs, shape=new_shape)\n",
        "\n",
        "        # so we need to reshape it to : [batch_size, nb_proj, seq_len, d_proj]\n",
        "        return tf.transpose(splited_inputs, perm=[0,2,1,3])\n",
        "\n",
        "    def call(self, queries, keys, values, mask):\n",
        "        batch_size = tf.shape(queries)[0]\n",
        "\n",
        "        queries = self.query_lin(queries)\n",
        "        keys = self.key_lin(keys)\n",
        "        values = self.value_lin(vlaues)\n",
        "\n",
        "        # 이제 우리는 프로즈를 만들기 위해 그것들을 나누었다.\n",
        "        queries = self.split_proj(queries, batch_size)\n",
        "        keys = self.split_proj(keys, batch_size)\n",
        "        values = self.split_proj(values, batch_size)\n",
        "\n",
        "        #each of the q, k , v are [batch_size, nb_proj, seq_len, d_proj]\n",
        "        attention = scaled_dot_product_attention(queries, keys, vlaues, mask)\n",
        "\n",
        "        # 이제 위에서 했던 갈라진 부분을 뒤집을 거야 : reshape + concat \n",
        "        attention = tf.greanspose(attention, perm=[0,2,1,3])\n",
        "        # we have [batch_size, seq_len, nb_proj, d_proj] so now we concat 2,3 \n",
        "        concat_attention = tf.reshapre(attention, shape=(batch_size, -1, self.d_model))\n",
        "        outputs = self.final_lin(concat_attention)\n",
        "        return outputs \n"
      ],
      "metadata": {
        "id": "bZojqtrvEr_J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### C - Encoder "
      ],
      "metadata": {
        "id": "BVBnYTrhVbCm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderLayer(layers.Layer):\n",
        "    \n",
        "    def __init__(self, FFN_units, nb_proj, dropout):\n",
        "        \"\"\"\n",
        "        :FFN_units: \n",
        "            feed forward networks units : the number of units for the\n",
        "            feed forward which you can see in the encoder part of the \n",
        "            paper ( right after the attention there is a feed forward...)\n",
        "            \n",
        "            \n",
        "\n",
        "        : nb_project:\n",
        "            the number of projections we have (8)\n",
        "\n",
        "            \n",
        "\n",
        "        : dropout : \n",
        "            the dropout rate e.g. 0.3 \n",
        "        \"\"\"\n",
        "\n",
        "        super(EncoderLayer, self).__init__()\n",
        "        self.FFN_units = FFN_units # hidden_layer의 수  #2048\n",
        "        self.nb_proj = nb_proj # # multi-head의 수 #8\n",
        "        self.dropout = dropout \n",
        "    \n",
        "    # 우리는 인코더를 만들 때 우리가 원하는 많은 바들이 없기 때문에 이것을 사용한다. 그래서 우리가 대신 '빌드' 함수를 사용할 때 우리는 그것들을 얻을 수 없다.\n",
        "    def build(self, input_shape):\n",
        "        self.d_model = input_shape[-1]\n",
        "        # 우리는 먼저 다중 헤드 어텐션을 위한 객체를 만든다.\n",
        "        self.multi_head_attention = MultiHeadAttention(self.nb_proj)\n",
        "        self.dropout_1 = layers.Dropout(rate=self.dropout)\n",
        "        self.norm_1 = layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "        self.dense_1 = layers.Dense(units = self.FFN_units, activation=\"relu\")\n",
        "        self.dense_2 = layers.Dense(units= self.d_model, activation=\"relu\")\n",
        "        self.dropout_2 = layers.Dropout(rate=self.dropout)\n",
        "        self.norm_2 = layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "    def call(self, inputs, mask, training):\n",
        "        \"\"\"\n",
        "        : mask: which we will apply in the multi-head attention\n",
        "        : training:\n",
        "            모델이 오버피팅되지 않도록 교육하는 동안 드롭아웃을 사용합니다=참\n",
        "            하지만 우리는 테스트만 할 때 그것을 사용하지 않습니다(일명 train=false).\n",
        "        \"\"\"\n",
        "        # 아키텍처를 살펴보면 인코더의 모든 쿼리/키/밸이 이전 계층에서 얻은 입력과 동일한 어레이임을 알 수 있습니다.\n",
        "        attention = self.multi_head_attention(inputs, inputs, inputs, mask)\n",
        "\n",
        "        # dropout + normalization after the attention\n",
        "        attention = self.dropout_1(attention, training = training)\n",
        "        # we do + inputs here 이유는 아키텍처에서 그것들이 결과 주의에 대한 이전 입력에 여전히 일치하기 때문에 우리는 그것을 정규화한다.\n",
        "        attention = self.norm_1(attention+inputs)\n",
        "\n",
        "        # now we do the dense in our FFN:\n",
        "        outputs = self.dense_1(attention)\n",
        "        outputs = self.dense_2(outputs)\n",
        "        outputs = self.dropout_2(outputs)\n",
        "        outputs = self.norm_2(outputs + attention)\n",
        "\n",
        "        return outputs \n",
        "\n",
        "\n",
        "    \n"
      ],
      "metadata": {
        "id": "4Mw3-CmzVcpX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(layers.Layer):\n",
        "    \n",
        "    def __init__(self,\n",
        "                 nb_encoding_layers,\n",
        "                 FFN_units, \n",
        "                 nb_proj,\n",
        "                 dropout,\n",
        "                 vocab_size,\n",
        "                 d_model,\n",
        "                 name=\"encoder\"):\n",
        "        # we put name = name here because 이름이 레이어 클래스에 속하는 것이기 때문입니다. so we tell it to use name=\"encoder\"\n",
        "        super(Encoder, self).__init__(name=name)\n",
        "        self.nb_encoding_layers = nb_encoding_layers # 연속된 인코더 수  \n",
        "        self.d_model = d_model # 출력의 크기 e.g glove(200)\n",
        "\n",
        "        # 우리는 보캅에 사용되는 최대값을 알 수 있도록 보캅 크기를 제공한다.\n",
        "        # tf.keras.layers.Embedding(input_dim, output_dim) 양의 정수 (인덱스)를 고정 크기의 밀도가 높은 벡터로 바꿉니다. \n",
        "        # input_dim : 들어가는 숫자의 최댓값이 들어간다. \n",
        "        # output_dim : 결과값이 몇개로 나올지를 정한다. \n",
        "        self.embedding = layers.Embedding(vocab_size, d_model)\n",
        "        self.pos_encoding = PositionalEncoding()\n",
        "        self.dropout = layers.Dropout(rate=dropout)\n",
        "        #과적합 방지 \n",
        "\n",
        "        self.enc_layers = [EncoderLayer(FFN_units, nb_proj, dropout) for _ in range(self.nb_encoding_layers)]\n",
        "        \n",
        "    def call(self, inputs, mask, training):\n",
        "        # look at the paper's architecture while doing these embedding with maybe glove eights .. \n",
        "        outputs = self.embedding(inputs)\n",
        "        # 우리가 이것을 한 이유는 3.4항의 종이에 쓰인 것 때문에 그들은 그들이 곱하기라고 말했다. Embeddings and softmax\n",
        "        outputs *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "        #this will give us the concat : outputs + pos_encoding\n",
        "        outputs = self.pos_encoding(outputs)\n",
        "        #이제 우리는 모든 인코딩 레이어 전에 드롭아웃을 한다.\n",
        "        # we give it training=bool -> so dont do dropout when training=false\n",
        "        outputs = self.dropout(outputs, training)\n",
        "\n",
        "        # 지금은 EmbeddingLayer를 한 번이 아니라 여러 번 해요.\n",
        "        for i in range(self.nb_encoding_layers):\n",
        "            # 따라서 다음 매개 변수를 사용하여 각 (i)번째 인코더에 적용합니다.\n",
        "            outputs = self.enc_layers[i](outpus, mask, training)\n",
        "\n",
        "        return outputs\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "eHDQ7OowXecI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### D - Decoder "
      ],
      "metadata": {
        "id": "BN3LZIaIlgSO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderLayer(layers.Layer):\n",
        "    def __init__(self, FFN_units, nb_proj, dropout):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "        self.FFN_units = FFN_units\n",
        "        self.nb_proj = nb_proj\n",
        "        self.dropout = dropout \n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.d_model = input_shape[-1]\n",
        "\n",
        "        # MHA 1\n",
        "        self.multi_head_attention_1 = MultiHeadAttention(self.nb_proj)\n",
        "        self.dropout_1 = layers.Dropout(rate=self.dropout)\n",
        "        self.norm_1 = layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "        # MHA 2 \n",
        "        self.multi_head_attention_2 = MultiHeadAttention(self.nb_proj)\n",
        "        self.dropout_2 = layers.Dropout(rate=self.dropout)\n",
        "        self.norm_2 = layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "        #FFN\n",
        "        self.dense_1 = layers.Dense(units = self.FFN_units, activation='relu')\n",
        "        self.dense_2 = layers.Dense(units = self.d_model)\n",
        "        self.dropout_3 = layers.Dropout(rate = self.dropout)\n",
        "        self.norm_3 = layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "    \n",
        "    def call(self, inputs, enc_outputs, mask_1, mask_2, training):\n",
        "        # check the architecture in the paper to see why we do these\n",
        "\n",
        "        # this is the 1# attention \n",
        "        attention = self.multi_head_attention_1(inputs, inputs, inputs, mask_1)\n",
        "        # we give it training=bool -> so dont do dropout when training=false \n",
        "        attention = self.dropout_1(attention, training)\n",
        "        attention = self.norm_1(attention + inputs)\n",
        "\n",
        "        # this is the 2# attention. this is ALOT different than before one pay attention\n",
        "        attention_2 = self.multi_head_attention_2(attention,\n",
        "                                                  enc_outputs,\n",
        "                                                  enc_outputs,\n",
        "                                                  mask_2)\n",
        "        \n",
        "        #we give it training=bool -> so dont do dropout when training=fasle\n",
        "        attention_2 = self.dropout_2(attention_2, training)\n",
        "        attention_2 = self.norm_2(attention_2 + inputs)\n",
        "\n",
        "        # the denses \n",
        "        outputs = self.dense_1(attention_2)\n",
        "        outputs = self.dense_2(outputs)\n",
        "        outputs = self.dropout_3(outputs, training)\n",
        "        outputs = self.norm_3(outputs + attention_2)\n",
        "\n",
        "        return outputs \n",
        "\n",
        "        \n",
        "\n"
      ],
      "metadata": {
        "id": "riIFVwC9liRb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(layers.Layer):\n",
        "\n",
        "    def __init__(self,\n",
        "                 nb_decoding_layers,\n",
        "                 FFN_units,\n",
        "                 nb_proj,\n",
        "                 dropout,\n",
        "                 vocab_size,\n",
        "                 d_model,\n",
        "                 name=\"decoder\"):\n",
        "        super(Decoder, self).__init__(name=name)\n",
        "        self.nb_decoding_layers = nb_decoding_layers # the number of encoders in a row \n",
        "        self.d_model = d_model # the size of the output e.g glove(200)\n",
        "\n",
        "        # 우리는 보캅에 사용되는 최대 개수를 알 수 있도록 보캅 사이즈를 제공한다.\n",
        "        self.embedding = layers.Embedding(vocab_size, d_model)\n",
        "        self.pos_encoding = PositionalEncoding()\n",
        "        self.dropout = layers.Dropout(rate=dropout)\n",
        "\n",
        "        self.dec_layers = [DecoderLayer(FFN_units, nb_proj, dropout) for _ in range(nb_decoding_layers)]\n",
        "\n",
        "    def call(self, inputs, enc_outputs, mask_1, mask_2, training):\n",
        "        # look at the paper's architecture while doing these embedding with maybe glove weights ... \n",
        "        outputs = self.embedding(inputs)\n",
        "        # the reason why we did this was vecause of what was writtent on the paper in secssion 3.4  which they said they multiplied\n",
        "        # it by squt of d_model \n",
        "        outputs *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "        #this will give us the concat : outputs + pos_encoding\n",
        "        outputs = self.pos_encoding(outputs)\n",
        "        #now we do dropout before all the encoding layers \n",
        "        # we give it training=bool -> so dont do dropout when training=false \n",
        "        outputs = self.dropout(outputs, training)\n",
        "\n",
        "        # now we do the EmbeddingLayer a couple of times, not just once\n",
        "        for i in range(self, nb_decoding_layers):\n",
        "            # so we apply it to the (i)th encoder in each for with these params:\n",
        "            outputs = self.dec_layers[i](outputs,\n",
        "                                         enc_outputs,\n",
        "                                         mask_1,\n",
        "                                         mask_2, \n",
        "                                         training)\n",
        "        return outputs \n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "_eMv8gVy0FV6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### E - Transformer "
      ],
      "metadata": {
        "id": "kBxCV8lq7mzF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer(tf.keras.Model):\n",
        "\n",
        "    def __init__(self,\n",
        "                 vocab_size_enc,\n",
        "                 vocab_size_dec,\n",
        "                 d_model,\n",
        "                 nb_layers,\n",
        "                 FFN_units,\n",
        "                 nb_proj,\n",
        "                 dropout,\n",
        "                 name=\"transformer\"):\n",
        "        super(Transformer, self).__init__(name=name)\n",
        "\n",
        "        #initing the Objects \n",
        "        self.encoder = Encoder(nb_layers,\n",
        "                               FFN_units,\n",
        "                               nb_proj,\n",
        "                               dropout,\n",
        "                               vocab_size_enc,\n",
        "                               d_model)\n",
        "        \n",
        "        self.decoder = Decoder(nb_layers,\n",
        "                               FFN_units,\n",
        "                               nb_proj,\n",
        "                               dropout,\n",
        "                               vocab_size_dec,\n",
        "                               d_model)\n",
        "        \n",
        "        # 이것은 당신이 크기 vocab_dec의 enc&dec 출력 의지를 조합한 후 가장 마지막에 있다.\n",
        "        # tf.keras.layers.Dense 이건 바로 신경망을 만드는 것\n",
        "        # units = 출력 값의 크기\n",
        "        self.last_linear = layers.Dense(units = vocab_size_dec)\n",
        "\n",
        "    def create_padding_mask(self, seq):\n",
        "        \"\"\"\n",
        "        : seq: [batch_size, seq_len(20)]\n",
        "        : reuturn : \n",
        "        \"\"\"\n",
        "        # so this gives us element wise equal true/false with broadcasting on 0 \n",
        "        # so we just want to see which words dont exist to give it true in all the batches \n",
        "        mask = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
        "        # so now we will return that mask we made but with 2 broadcasted new dimensions so it can match the input needed in attention \n",
        "        # (in the next cell I made and example to see it better)\n",
        "        return mask[:, tf.newaxis, tf.newaxis, :]\n",
        "\n",
        "    def create_look_ahead_mask(self, seq):\n",
        "        \"\"\"\n",
        "        : seq: [batch_size, seq_len(20)]\n",
        "        : return : \n",
        "        \"\"\"\n",
        "        seq_len = tf.shape(seq)[1]\n",
        "        #sample of what it produces is in the cell below \n",
        "        #why do we do this?  because when we predict the ith word we dont see words from \n",
        "        look_ahead_mask = 1 -tf.linalg.band_part(tf.ones((seq_len,seq_len)),-1,0)\n",
        "        return look_ahead_mask\n",
        "\n",
        "    def call(self, enc_inputs, dec_inputs, training):\n",
        "        # 여기서 인코더와 디코더 및 마스크 결합\n",
        "\n",
        "        # 인코더용 마스크 생성\n",
        "        enc_mask = self.create_padding_mask(enc_inputs)\n",
        "\n",
        "        # 디코더용 마스크 생성\n",
        "\n",
        "        #마스크 #1은 출력, 출력, 출력을 q/k/v로 사용하는 첫 번째 디코더 어텐션용이므로 두 개의 마스크 중 최대값을 얻을 수 있습니다.\n",
        "        dec_mask_1 = tf.maximum(self.create_padding_mask(dec_inputs),\n",
        "                                self.create_look_ahead_mask(dec_inputs))\n",
        "        # 마스크 #2는 인코더의 출력을 v/k로 사용하는 두 번째 디코더 어텐션으로, 나중에 q*k를 할 때 *v를 할 때 올바른 출력을 얻을 수 있도록 입력에 마스킹을 해야 합니다.\n",
        "        # 그러나 비디오가 말한 대로입니다. 그러나 나중에 none을 사용하면 실제로 입력을 사용하지 않기 때문에 더 정확하게 시도하지 않습니다.그러나 이전 관심에서 이미 마스킹되고 처리된 출력\n",
        "        dec_mask_2 = self.create_padding_mask(enc_inputs)\n",
        "\n",
        "        enc_outputs = self.encoder(enc_inputs, enc_mask, training)\n",
        "        dec_outputs = self.decoder(dec_inputs,\n",
        "                                   enc_outputs,\n",
        "                                   dec_mask_1,\n",
        "                                   dec_mask_2,\n",
        "                                   training)\n",
        "        \n",
        "        outputs = self.last_linear(dec_outputs)\n",
        "        \n",
        "        return outputs\n",
        "\n",
        "\n",
        "\n",
        "        \n",
        "\n",
        "        \n"
      ],
      "metadata": {
        "id": "uvJwM5_C7o46"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### testing masks to see their ouputs \n"
      ],
      "metadata": {
        "id": "htmcbAyGBrUx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_padding_mask(seq):\n",
        "    mask = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
        "    return mask[:, tf.newaxis, tf.newaxis, :]\n",
        "\n",
        "def create_look_ahead_mask(seq):\n",
        "    seq_len = tf.shape(seq)[1]\n",
        "    look_ahead_mask = 1 - tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0)\n",
        "    return look_ahead_mask"
      ],
      "metadata": {
        "id": "9wRP-_IeBufT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# sample: 1 batch of seq_len=8 ->[1,8] this will become [1,1,1,8] then\n",
        "# broadcasting will happen for later stages\n",
        "# mask=1 means we should delete this\n",
        "seq = tf.cast([[1, 2, 3, 0, 4, 0, 0, 0]], tf.int32)\n",
        "\n",
        "print('This padding masking as the name says shows which words exist: (later broadcasting will happen for each batch & nb_proj & d_proj')\n",
        "print(create_padding_mask(seq), end='\\n\\n')\n",
        "\n",
        "print(\"This look ahead masking as the name says shows that only for i>=j we need to keep them (=0's), so we should not see the feature indxes (i<j) (the 1's)\")\n",
        "print('Have in mind that mask=1 means we need to get rid of that, dont confuse it with mask=0')\n",
        "print(1 - tf.linalg.band_part(tf.ones((5, 5)), -1, 0), end='\\n\\n')\n",
        "\n",
        "print('Now applying both: (pay very close attention to this samples output, very important)')\n",
        "print(tf.maximum(create_padding_mask(seq),\n",
        "                 create_look_ahead_mask(seq)))"
      ],
      "metadata": {
        "id": "jwCZY_TxMYi5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 4 : Training \n",
        "- Parameters "
      ],
      "metadata": {
        "id": "qlZw6iLvMbLv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- d_model = 512\n",
        "    1. embedding 차원\n",
        "    2. encoder와 decoder에서 정해진 입출력의 크기\n",
        "    3. 각 encode와 decoder가 다음 층으로 보낼 때도 동일한 차원을 유지 \n",
        "- num_layers = 6 \n",
        "    1. encoder , decoder의 layer 수 \n",
        "\n",
        "- num_heads = 8 \n",
        "    1. attention 사용 시 , 1번 하는 것 보다 여러 개로 분할해서 병렬로 attentiond을 수행 \n",
        "    2. 결과값을 다시 하나로 합치는 방식 \n",
        "    3. 이 때 병렬의 개수\n",
        "\n",
        "- FFN = 2048\n",
        "    1. hidden layer의 크기 \n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "8Tr-vYt-RHeT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tf.keras.backend.clear_session()\n",
        "\n",
        "# Hyper-parameters:\n",
        "EPOCHS = 2\n",
        "D_MODEL = 128 # 512 takes more time but has a lot better results\n",
        "NB_LAYERS = 4 # 6\n",
        "FFN_UNITS = 512 # 2048\n",
        "NB_PROJ = 8 # 8\n",
        "DROPOUT = 0.1 # 0.1\n",
        "\n",
        "transformer = Transformer(vocab_size_enc=VOCAB_SIZE_EN,\n",
        "                          vocab_size_dec=VOCAB_SIZE_DE,\n",
        "                          d_model=D_MODEL,\n",
        "                          nb_layers=NB_LAYERS,\n",
        "                          FFN_units=FFN_UNITS,\n",
        "                          nb_proj=NB_PROJ,\n",
        "                          dropout=DROPOUT)"
      ],
      "metadata": {
        "id": "BUOBQC_FN3Kd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loss"
      ],
      "metadata": {
        "id": "lvePSU5cOCmi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# loss\n",
        "\n",
        "# 1) example of SparseCategoricalCrossentropy:\n",
        "# y_true = [1, 2]\n",
        "# y_pred = [[0.05, 0.95, 0], [0.1, 0.8, 0.1]]\n",
        "# 2) we made from_logits=true. why? 소프트맥스가 적용되지 않은 숫자를 준다.\n",
        "# 3) we made reduction='none'. why? 일반적으로 모든 배치에서 발생한 모든 손실을 합하여 하나의 숫자로 만들지만, 우리는 그것을 원하지 않습니다. 우리는 패딩된 손실을 제거하고 싶습니다.\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True,\n",
        "                                                            reduction='none')\n",
        "def loss_function(target, pred):\n",
        "    # so by using this mask we will get rid of all the losses that \n",
        "    # corrispond to 0 in our target (aka. y_true)\n",
        "    mask = tf.math.logical_not(tf.math.equal(target, 0)) # [326, 4, 0] -> [1, 1, 0]\n",
        "    loss_ = loss_object(target, pred) # we got the loss numbers (not their softmax probabilities)\n",
        "    \n",
        "    # make sure that both loss_ and mask have the same data type so we can mult them\n",
        "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "    # we make the computed losses for 0's 0\n",
        "    loss_ *= mask\n",
        "    \n",
        "    # compute the mean loss and return\n",
        "    return tf.reduce_mean(loss_)\n",
        "\n",
        "# 주어진 값의 (가중) 평균을 계산합니다.,  훈련 중 손실을 추적합니다, \n",
        "train_loss = tf.keras.metrics.Mean(name=\"train_loss\")\n",
        "# 예측이 정수 레이블과 일치하는 빈도를 계산합니다., 훈련 중에 발생하는 문제를 추적합니다.\n",
        "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')"
      ],
      "metadata": {
        "id": "wvEeppjiOHN0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Optimizer "
      ],
      "metadata": {
        "id": "X8avf4FiOH_g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
        "\n",
        "    def __init__(self, d_model, warmup_steps=4000):\n",
        "        super(CustomSchedule, self).__init__()\n",
        "        \n",
        "        self.d_model = tf.cast(d_model, tf.float32)\n",
        "        self.warmup_steps = warmup_steps\n",
        "\n",
        "   # call 대신 __call_을 사용했으며 매개 변수 단계는 제공하지 않으며 tf.keras.optimizers.promizers에서 가져옵니다.LearningRateSchedule 자체\n",
        "    def __call__(self, step):\n",
        "        # read this part in paper and you'll understand arg1 & arg2 which they\n",
        "        # used in their custom learning rate\n",
        "        arg1 = tf.math.rsqrt(step)\n",
        "        arg2 = step * (self.warmup_steps**-1.5)\n",
        "\n",
        "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n",
        "\n",
        "learning_rate = CustomSchedule(D_MODEL)\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate,\n",
        "                                     beta_1=0.9,\n",
        "                                     beta_2=0.98,\n",
        "                                     epsilon=1e-9)"
      ],
      "metadata": {
        "id": "TpkE6N6JOJ4L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Checkpoints (delete your checkpoints if there are some unexpected errors when changing your data)"
      ],
      "metadata": {
        "id": "ve8Uv5OjOM5C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# making a checkpoint:\n",
        "checkpoint_path = \"./ckpt/\"\n",
        "\n",
        "ckpt = tf.train.Checkpoint(transformer=transformer,\n",
        "                           optimizer=optimizer)\n",
        "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n",
        "\n",
        "# lets check if we already have a checkpoint\n",
        "if ckpt_manager.latest_checkpoint:\n",
        "    ckpt.restore(ckpt_manager.latest_checkpoint)\n",
        "    print(\"Latest Checkpoint Restored...\")"
      ],
      "metadata": {
        "id": "zv7rf8dVOORz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Epochs"
      ],
      "metadata": {
        "id": "Amb_4BmfOPvu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(EPOCHS):\n",
        "    print(\"Start of epoch {}\".format(epoch+1))\n",
        "    start = time.time()\n",
        "\n",
        "    train_loss.reset_states()\n",
        "    train_accuracy.reset_states()\n",
        "\n",
        "    # iterate on each batch:\n",
        "    for (batch_index, (enc_inputs, targets)) in enumerate(dataset):\n",
        "        # we take all the target minus the last word: <s> hello friend <e>. so we get rid of <s>.\n",
        "        # why? because we are trying to predict the next word each time, so at the last step\n",
        "        # we are predicting <e> and we are done, so we wont need it as an input for our decoder\n",
        "        dec_inputs = targets[:, :-1]\n",
        "        # we shift 1 to right because tokens are: <s> hello friend <e>. so we get rid of <s>.\n",
        "        # when we want to do the predictions, we wont need to predict the <s>, we start with <s>\n",
        "        dec_outputs_real = targets[:, 1:]\n",
        "\n",
        "        # this will record everything that happens when we do predictions\n",
        "        with tf.GradientTape() as tape:\n",
        "            # the true is for training\n",
        "            predictions = transformer(enc_inputs, dec_inputs, True)\n",
        "            loss = loss_function(dec_outputs_real, predictions)\n",
        "        \n",
        "        # now we get the gradients using this method using the tape\n",
        "        gradients = tape.gradient(loss, transformer.trainable_variables)\n",
        "        # now we apply the gradients according to our Adam optimizer\n",
        "        optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
        "\n",
        "        # now lets add our loss to the train loss object that keeps track of the loss\n",
        "        train_loss(loss)\n",
        "        train_accuracy(dec_outputs_real, predictions)\n",
        "\n",
        "        # now let's print our loss and acc from time to time.....\n",
        "        if batch_index % 50 == 0:\n",
        "            print(\"Epoch {} Batch {} Loss {:.4f} Accuracy {:.4f}\".format(\n",
        "                epoch+1, batch_index, train_loss.result(), train_accuracy.result()))\n",
        "\n",
        "    \n",
        "    # at the end of each epoch we save a checkpoint\n",
        "    ckpt_save_path = ckpt_manager.save()\n",
        "    print(\"Saved checkpoint for epoch {}!\".format(epoch+1))"
      ],
      "metadata": {
        "id": "wxKpgjJTOTaG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 5 : Evaluation "
      ],
      "metadata": {
        "id": "qhSPOgQIOVmY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(inp_sentence):\n",
        "    # turn the sentence to the tokenizer_encoded format [hi, bye] -> [241, 6]\n",
        "    inp_sentence = [VOCAB_SIZE_EN-2] + tokenizer_en.encode(inp_sentence) + [VOCAB_SIZE_EN-1]\n",
        "    # expand dim on axis=0 to simulate the batch dimmension\n",
        "    enc_input = tf.expand_dims(inp_sentence, axis=0)\n",
        "\n",
        "    # let's make the ouput which starts with <s> and add that axis=0 for batch=0\n",
        "    output = tf.expand_dims([VOCAB_SIZE_DE-2], axis=0)\n",
        "\n",
        "    # the loop to predict the next word of output each time and output += it\n",
        "    for _ in range(MAX_LENGTH):\n",
        "        # we put false because we are not training so no dropout\n",
        "        # predictions = [btch_sz=1, seq_len(output_so_far), vocav_sz_de(the \n",
        "        # softmax values of each word, the higher the number the higher the \n",
        "        # probability for that word)]\n",
        "        predictions = transformer(enc_input, output, False)\n",
        "        # we want to take the last word of this prediction\n",
        "        prediction = predictions[:, -1:, :]\n",
        "        # we do argmax to get the index of the most probable next word\n",
        "        predicted_id = tf.cast(tf.argmax(prediction, axis=-1), tf.int32)\n",
        "\n",
        "        # we reached the end of the sentence\n",
        "        if predicted_id == VOCAB_SIZE_DE-1:\n",
        "            return tf.squeeze(output, axis=0)\n",
        "        \n",
        "        # now we know add the new prediction to the last of the output\n",
        "        output = tf.concat([output, predicted_id], axis=-1)\n",
        "    \n",
        "    #even if we didn't reach the end of the sentence we can't continue\n",
        "    return tf.squeeze(output, axis=0)"
      ],
      "metadata": {
        "id": "skW4N5fSOae4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def translate(sentence):\n",
        "    output = evaluate(sentence).numpy()\n",
        "    # get rid of <s> and <e> if they exist\n",
        "    output = [i for i in output if i < VOCAB_SIZE_DE-2]\n",
        "    # decode indexes to words e.g. [241, 6] -> [hi, bye] \n",
        "    predicted_sentence = tokenizer_de.decode(output)\n",
        "    print(\"Input: {}\".format(sentence))\n",
        "    print(\"Predicted translation: {}\".format(predicted_sentence))"
      ],
      "metadata": {
        "id": "oKL8tYTYOcNQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "translate(\"This is a great day!\")"
      ],
      "metadata": {
        "id": "ebz7BZzjOdf1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}